# -*- coding: utf-8 -*-
"""Coding_Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zfx-TtLMIiKd2kTac6cV1siQ4K2JMRjy
"""

import tensorflow as tf
print(tf.__version__)

"""# Model subclassing and custom training loops

## Coding tutorials
 #### [1. Model subclassing](#coding_tutorial_1)
 #### [2. Custom layers](#coding_tutorial_2)
 #### [3. Automatic differentiation](#coding_tutorial_3)
 #### [4. Custom training loops](#coding_tutorial_4)
 #### [5. tf.function decorator](#coding_tutorial_5)
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense

class MyModel(Model):
	    def __init__(self, units, classes, **kwargs):
	        super(MyModel, self).__init__(**kwargs)
	        self.d1 = Dense(units, activation='relu')
	        self.d2 = Dense(classes, activation='softmax')

	    def call(self, x):  
	        x = self.d1(x)
	        return self.d2(x)
    
mymodel = MyModel(16, 4, name='my_model')
mymodel.compile(loss='binary_crossentropy')
mymodel.build( (128, 8))

mymodel.summary()

"""***
<a id="coding_tutorial_1"></a>
## Model subclassing
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate

"""#### Create a simple model using the model subclassing API"""

# Build the model

class MyModel(Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.dense_1 = Dense(64, activation='relu')
        self.dense_2 = Dense(10)

    def call(self, inputs):
        x = self.dense_1(inputs)
        x = self.dense_2(x)
        return x

class MyModel2(Model):
    def __init__(self):
        super(MyModel2, self).__init__()
        self.dense_1 = Dense(64, activation='relu')
        self.dense_2 = Dense(10)
        self.dropout = Dropout(.5)

    def call(self, inputs, training=True):
        x = self.dense_1(inputs)
        if training:
            x = self.dropout(x)
        x = self.dense_2(x)
        return x

class MyModel3(Model):
    def __init__(self):
        super(MyModel3, self).__init__()
        self.dense_1 = Dense(64, activation='relu')
        self.dense_2 = Dense(10)
        self.dense_3 = Dense(5)
        self.softmax = Softmax()

    def call(self, inputs):
        x = self.dense_1(inputs)
        y1 = self.dense_2(inputs)
        y2 = self.dense_3(y1)

        concat = concatenate([x,y2])
        return self.softmax(concat)

# Print the model summary

model = MyModel()
model(tf.random.uniform([1, 10]))
model.summary()

model2 = MyModel2()
model2(tf.random.uniform([1,10]))
model2.build((1,10))
model2.summary()

model3 = MyModel3()
model3(tf.random.uniform([1,10]))
model3.summary()

"""***
<a id="coding_tutorial_2"></a>
## Custom layers
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Softmax

"""#### Create custom layers"""

# Create a custom layer

class MyLayer(Layer):

    def __init__(self, units, input_dims):
        super(MyLayer, self).__init__()
        self.w = self.add_weight(shape=(input_dims, units), initializer='random_normal')
        self.b = self.add_weight(shape=(units,), initializer='zeros')

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

dense_layer = MyLayer(3, 5)
x = tf.ones((1,5))       
print(x)
print(dense_layer.weights) 
print(dense_layer(x))

print('trainable weights:', len(dense_layer.trainable_weights))
print('non-trainable weights:', len(dense_layer.non_trainable_weights))

# Specify trainable weights

class MyLayer(Layer):

    def __init__(self, units, input_dims):
        super(MyLayer, self).__init__()
        self.w = self.add_weight(shape=(input_dims, units), 
                                 initializer='random_normal', trainable=False)
        self.b = self.add_weight(shape=(units,), initializer='zeros', trainable=False)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

dense_layer = MyLayer(3, 5)

print('trainable weights:', len(dense_layer.trainable_weights))
print('non-trainable weights:', len(dense_layer.non_trainable_weights))

# Create a custom layer to accumulate means of output values

class MyLayerMean(Layer):

    def __init__(self, units, input_dims):
        super(MyLayerMean, self).__init__()
        self.w = self.add_weight(shape=(input_dims, units), initializer='random_normal')
        self.b = self.add_weight(shape=(units,), initializer='zeros')
        self.sum_activation = tf.Variable(initial_value=tf.zeros((units,)), 
                                          trainable=False)
        self.number_calls = tf.Variable(initial_value=0, trainable=False)

    def call(self, inputs):
        activations = tf.matmul(inputs, self.w) + self.b
        self.sum_activation.assign_add(tf.reduce_sum(activations, axis=0))
        self.number_calls.assign_add(inputs.shape[0])
        return activations, self.sum_activation / tf.cast(self.number_calls, tf.float32)

dense_layer = MyLayerMean(3, 5)

# Test the layer

y, activation_means = dense_layer(tf.ones((1, 5)))
print(activation_means.numpy())

y, activation_means = dense_layer(tf.ones((2, 5)))
print(activation_means.numpy())

# Create a Dropout layer as a custom layer

class MyDropout(Layer):

    def __init__(self, rate):
        super(MyDropout, self).__init__()
        self.rate = rate
        
    def call(self, inputs):
        # Define forward pass for dropout layer
        return tf.nn.dropout(inputs, rate=self.rate)


class MyLayer(Layer):

    def __init__(self, units, input_dims):
        super(MyLayer, self).__init__()
        self.w = self.add_weight(shape=(input_dims, units), 
                                 initializer='random_normal', trainable=False)
        self.b = self.add_weight(shape=(units,), 
                                 initializer='zeros', trainable=False)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

"""#### Implement the custom layers into a model"""

# Build the model using custom layers with the model subclassing API

class MyModel(Model):

    def __init__(self, units_1, input_dim_1, units_2, units_3):
        super(MyModel, self).__init__()
        # Define layers
        self.layer_1 = MyLayer(units_1, input_dim_1)
        self.dropout_1 = MyDropout(0.5)
        self.layer_2 = MyLayer(units_2, units_1)
        self.dropout_2 = MyDropout(0.5)
        self.layer_3 = MyLayer(units_3, units_2)
        self.softmax = Softmax()

    def call(self, inputs):
        # Define forward pass
        h = self.layer_1(inputs)
        # h = tf.nn.relu(h)
        # h = self.dropout_1(h)

        # h = self.layer_2(h)
        # h = tf.nn.relu(h)
        # h = self.dropout_2(h)
        
        # h = self.layer_3(h)
        # h = self.softmax(h)
        return h

type(tf.ones((1, 10000)))

# Instantiate a model object

model = MyModel(64,10000,64,46)
print(model(tf.ones((1, 10000))))
# model.summary()

"""***
<a id="coding_tutorial_3"></a>
## Automatic differentiation
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

"""#### Create synthetic data"""

# Create data from a noise contaminated linear model

def MakeNoisyData(m, b, n=20):
    x = tf.random.uniform(shape=(n,))
    noise = tf.random.normal(shape=(len(x),), stddev=0.1)
    y = m * x + b + noise
    return x, y

m=1
b=2
x_train, y_train = MakeNoisyData(m,b)
plt.plot(x_train, y_train, 'b.')

"""#### Define a linear regression model"""

from tensorflow.keras.layers import Layer

# Build a custom layer for the linear regression model

class LinearLayer(Layer):

    def __init__(self):
        super(LinearLayer, self).__init__()

        self.w = self.add_weight(shape=(1,), 
                                 initializer=tf.keras.initializers.RandomNormal())
        self.b = self.add_weight(shape=(1,), 
                                 initializer=tf.keras.initializers.zeros())
        
    def call(self, inputs):
        return self.w * inputs + self.b

linear_regression = LinearLayer()
print(f'Linear reg : {linear_regression(x_train)}')
print(f'Linear weights : {linear_regression.weights}')

"""#### Define the loss function"""

# Define the mean squared error loss function

def SquaredError(y_pred, y_true):
    return tf.reduce_mean(tf.square(y_pred - y_true)) 

starting_loss = SquaredError(linear_regression(x_train), y_train)
print("Starting loss", starting_loss.numpy())

"""#### Train and plot the model"""

# Implement a gradient descent training loop for the linear regression model

learningRate = 0.05
steps = 20

print(linear_regression.trainable_variables)

for i in range(steps):
    with tf.GradientTape() as tape:
        pred = linear_regression(x_train)
        loss = SquaredError(pred, y_train)

    if i == 1:
        print('LOSS :',loss.numpy())
        print('VARS :',[var.numpy() for var in linear_regression.trainable_variables])
    grad = tape.gradient(loss, linear_regression.trainable_variables)
    
    if i == 0:
        print('gradients', grad)

    linear_regression.w.assign_sub(learningRate * grad[0])
    linear_regression.b.assign_sub(learningRate * grad[1])

    print(f'Step : {i}, Loss : {loss.numpy()}')

# Plot the learned regression model

print("m:{},  trained m:{}".format(m,linear_regression.w.numpy()))
print("b:{},  trained b:{}".format(b,linear_regression.b.numpy()))

plt.plot(x_train, y_train, 'b*')

x_linear_regression=np.linspace(min(x_train), max(x_train),50)
plt.plot(x_linear_regression, linear_regression.w*x_linear_regression+linear_regression.b, 'r.')

"""***
<a id="coding_tutorial_4"></a>
## Custom training loops
"""

import tensorflow as tf 
import numpy as np
import matplotlib.pyplot as plt
import time

"""#### Build the model"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Softmax

# Define the custom layers and model

class MyLayer(Layer):

    def __init__(self, units, input_dims):
        super(MyLayer, self).__init__()
        self.w = self.add_weight(shape=(input_dims, units), 
                                 initializer='random_normal')
        self.b = self.add_weight(shape=(units,), 
                                 initializer='zeros')

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

class MyDropout(Layer):

    def __init__(self, rate):
        super(MyDropout, self).__init__()
        self.rate = rate
        
    def call(self, inputs):
        # Define forward pass for dropout layer
        return tf.nn.dropout(inputs, rate=self.rate)     

class MyModel(Model):

    def __init__(self, units_1, input_dim_1, units_2, units_3):
        super(MyModel, self).__init__()
        # Define layers
        self.layer_1 = MyLayer(units_1, input_dim_1)
        self.dropout_1 = MyDropout(0.5)
        self.layer_2 = MyLayer(units_2, units_1)
        self.dropout_2 = MyDropout(0.5)
        self.layer_3 = MyLayer(units_3, units_2)
        self.softmax = Softmax()

    def call(self, inputs):
        # Define forward pass
        h = self.layer_1(inputs)
        h = tf.nn.relu(h)
        h = self.dropout_1(h)

        h = self.layer_2(h)
        h = tf.nn.relu(h)
        h = self.dropout_2(h)
        
        h = self.layer_3(h)
        h = self.softmax(h)
        return h

model = MyModel(64, 10000, 64, 46)
print(model(tf.ones((1, 10000))))
model.summary()

"""#### Load the reuters dataset and define the class_names"""

# Load the dataset

from tensorflow.keras.datasets import reuters

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',
   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',
   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',
   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',
   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']

# print(len(class_names))
# np.unique(test_labels)
# train_labels = to_categorical(train_labels)
# test_labels = to_categorical(test_labels)

np.argmax(train_labels[0])

# Print the class of the first sample

print("Label: {}".format(class_names[(train_labels[0])]))

"""#### Get the dataset word index"""

print('len : ', len(train_data[0]))
print(train_data[0])

# Load the Reuters word index

word_to_index = reuters.get_word_index()

invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])
text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])

# Print the first data example sentence

print(len(text_news))
print(text_news)

"""#### Preprocess the data"""

# Define a function that encodes the data into a 'bag of words' representation

def bag_of_words(text_samples, elements=10000):
    output = np.zeros((len(text_samples), elements))
    for i, word in enumerate(text_samples):
        output[i, word] = 1.
    return output

x_train = bag_of_words(train_data)
x_test = bag_of_words(test_data)

print("Shape of x_train:", x_train.shape)
print("Shape of x_test:", x_test.shape)

"""#### Define the loss function and optimizer"""

# Define the categorical cross entropy loss and Adam optimizer

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

def loss(model, x, y, wd):
    kernel_variables = []
    for l in model.layers:
        for w in l.weights:
            if 'kernel' in w.name:
                kernel_variables.append(w)
    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])
    y_ = model(x)
    return loss_object(y_true=y, y_pred=y_) + wd_penalty

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

"""#### Train the model"""

# Define a function to compute the forward and backward pass
@tf.function
def grad(model, inputs, targets, wd):
    with tf.GradientTape() as tape:
        loss_value = loss(model, inputs, targets, wd)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)

# a = train_dataset.take(1)
# for x, y in a:
#     print(x)

# Implement the training loop

from tensorflow.keras.utils import to_categorical

start_time = time.time()

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, train_labels))
train_dataset = train_dataset.batch(32)

# keep results for plotting
train_loss_results = []
train_accuracy_results = []

no_epochs = 10
weight_decay = .005

for epoch in range(no_epochs):

    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.Accuracy()

    i=0
    for x, y in train_dataset:
        # optimize the model
        loss_value, grads = grad(model, x, y, weight_decay)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # compute the current loss
        epoch_loss_avg(loss_value)

        # compare pred label to actual label
        epoch_accuracy(to_categorical(y, num_classes=46), model(x))
        # epoch_accuracy(model(x), to_categorical(y))

    train_loss_results.append(epoch_loss_avg.result())
    train_accuracy_results.append(epoch_accuracy.result())

    print('Epoch : {}, Loss : {:.03f}, Accuracy : {:3.3%}'.format(
        epoch, epoch_loss_avg.result(), epoch_accuracy.result().numpy()
    ))
    
print("Duration :{:.3f}".format(time.time() - start_time))

"""#### Evaluate the model"""

# Create a Dataset object for the test set

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))
test_dataset = test_dataset.batch(32)

# Collect average loss and accuracy

epoch_loss_avg = tf.keras.metrics.Mean()
epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()

# Loop over the test set and print scores

from tensorflow.keras.utils import to_categorical

for x, y in test_dataset:
    # Optimize the model
    loss_value = loss(model, x, y, weight_decay)    
    # Compute current loss
    epoch_loss_avg(loss_value)  
    # Compare predicted label to actual label
    epoch_accuracy(to_categorical(y), model(x))

print("Test loss: {:.3f}".format(epoch_loss_avg.result().numpy()))
print("Test accuracy: {:.3%}".format(epoch_accuracy.result().numpy()))

"""#### Plot the learning curves"""

# Plot the training loss and accuracy

fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))
fig.suptitle('Training Metrics')

axes[0].set_ylabel("Loss", fontsize=14)
axes[0].plot(train_loss_results)

axes[1].set_ylabel("Accuracy", fontsize=14)
axes[1].set_xlabel("Epoch", fontsize=14)
axes[1].plot(train_accuracy_results)
plt.show()

"""#### Predict from the model"""

# Get the model prediction for an example input

predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]
print("Prediction: {}".format(class_names[predicted_label]))
print("     Label: {}".format(class_names[train_labels[0]]))

"""***
<a id="coding_tutorial_5"></a>
## tf.function decorator
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Softmax
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import reuters
import numpy as np
import matplotlib.pyplot as plt
import time

"""#### Build the model"""

# Initialize a new model

model = MyModel(64,10000, 64,46)

"""#### Redefine the grad function using the @tf.function decorator"""

# Use the @tf.function decorator

@tf.function
def grad(model, inputs, targets, wd):
    with tf.GradientTape() as tape:
        loss_value = loss(model, inputs, targets, wd)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)

"""#### Train the model"""

# Re-run the training loop

# Implement the training loop

from tensorflow.keras.utils import to_categorical

start_time = time.time()

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, train_labels))
train_dataset = train_dataset.batch(32)

# keep results for plotting
train_loss_results = []
train_accuracy_results = []

no_epochs = 10
weight_decay = .005

for epoch in range(no_epochs):

    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.Accuracy()

    i=0
    for x, y in train_dataset:
        # optimize the model
        loss_value, grads = grad(model, x, y, weight_decay)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # compute the current loss
        epoch_loss_avg(loss_value)

        # compare pred label to actual label
        epoch_accuracy(to_categorical(y, num_classes=46), model(x))

    train_loss_results.append(epoch_loss_avg.result())
    train_accuracy_results.append(epoch_accuracy.result())

    print('Epoch : {}, Loss : {}, Accuracy {:.3%}'.format(
        epoch, epoch_loss_avg.result(), epoch_accuracy.result().numpy()
    ))
    
print("Duration :{:.3f}".format(time.time() - start_time))

"""#### Print the autograph code"""

# Use tf.autograph.to_code to see the generated code

print(tf.autograph.to_code(grad.python_function))

tf.keras.utils.plot_model(model, show_shapes=True)

