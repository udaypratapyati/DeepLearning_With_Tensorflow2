# -*- coding: utf-8 -*-
"""Coding_Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17QsEd4GBSBnJSiwbbESDFuuaQS9huzWj
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)

print('GPU name: {}'.format(tf.test.gpu_device_name()))

"""# Sequence modelling 

## Coding tutorials
 #### 1.  The IMDb dataset
 #### 2. Padding and masking sequence data
 #### 3. The `Embedding` layer
 #### 4. The Embedding Projector
 #### 5. Recurrent neural network layers
 #### 6. Stacked RNNs and the `Bidirectional` wrapper

***
<a id="coding_tutorial_1"></a>
## The IMDb Dataset

#### Load the IMDB review sentiment dataset
"""

# Import imdb

import tensorflow.keras.datasets.imdb as imdb

# Download and assign the data set using load_data()

(x_train, y_train),(x_test, y_test) = imdb.load_data()

"""#### Inspect the dataset"""

# Inspect the type of the data

type(x_train)

# Inspect the shape of the data

print(f'x_train : {x_train.shape} - y_train : {y_train.shape}')
print(f'x_test : {x_test.shape} - y_test : {y_test.shape}')

# Display the first dataset element input
# Notice encoding

x_train[0]

# Display the first dataset element output

y_train[0]

"""#### Load dataset with different options"""

# Load the dataset with defaults

imdb.load_data(path='imdb.npz', index_from=3)
# ~/.keras/dataset/

# Limit the vocabulary to the top 500 words using num_words

imdb.load_data(num_words=1000)

# Ignore the top 10 most frequent words using skip_top

imdb.load_data(skip_top=10, num_words=1000, oov_char=2)

# Limit the sequence lengths to 500 using maxlen

imdb.load_data(maxlen=500)

# Use '1' as the character that indicates the start of a sequence

 imdb.load_data(start_char=1)

"""#### Explore the dataset word index"""

# Load the imdb word index using get_word_index()

imdb_word_index = imdb.get_word_index()

# View the word index as a dictionary,
# accounting for index_from.

index_from = 3
imdb_word_index = {key:value + index_from for key, value in imdb_word_index.items()}

# Retrieve a specific word's index

imdb_word_index['india']

# the is the most frequent word in this dataset, lets see.. index should be 4 -> start:1 + index_from:3
imdb_word_index['the']

# View an input sentence

inv_imdb_word_index = {value:key for key, value in imdb_word_index.items()}
[inv_imdb_word_index[index] for index in x_train[0] if index > index_from]

# Get the sentiment value

y_train[0]

"""---
<a id="coding_tutorial_2"></a>
## Padding and Masking Sequence Data
"""

# Load the imdb data set

(x_train, y_train),(x_test, y_test) = imdb.load_data()

"""#### Preprocess the data with padding"""

# Inspect the input data shape

print(f'x_train : {x_train.shape} - y_train : {y_train.shape}')
print(f'x_test : {x_test.shape} - y_test : {y_test.shape}')

# Pad the inputs to the maximum length using maxlen

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_x_train = pad_sequences(x_train, maxlen=300, padding='post', truncating='pre')

# Inspect the output data shape

padded_x_train.shape

"""#### Create a Masking layer"""

# Import numpy 

import numpy as np

dummy = padded_x_train[..., np.newaxis]
dummy.shape

# Masking expects to see (batch, sequence, features)
# Create a dummy feature dimension using expand_dims

padded_x_train = np.expand_dims(padded_x_train, -1)
padded_x_train.shape

# Create a Masking layer 

from tensorflow.keras.layers import Masking

masking_layer = Masking(mask_value=0.0)
tf_x_train = tf.convert_to_tensor(padded_x_train, dtype=tf.float32)

# Pass tf_x_train to it

masked_x_train = masking_layer(tf_x_train)

# Look at the dataset

masked_x_train[0]

# Look at the ._keras_mask for the dataset

# x_train._keras_mask # this will give error as it doesnot have a masking layer associated with it
masked_x_train._keras_mask

"""***
<a id="coding_tutorial_3"></a>
## The Embedding layer

#### Create and apply an `Embedding` layer
"""

# Create an embedding layer using layers.Embedding
# Specify input_dim, output_dim, input_length

embedding_layer = tf.keras.layers.Embedding(input_dim=501, output_dim=16)

# Inspect an Embedding layer output for a fixed input
# Expects an input of shape (batch, sequence, feature)

sequence_of_indices = tf.constant([[[0],[1],[5],[500]]])
sequence_of_embeddings = embedding_layer(sequence_of_indices)
sequence_of_embeddings

# Inspect the Embedding layer weights using get_weights()

embedding_layer.get_weights()[0]

# Get the embedding for the 14th index

embedding_layer.get_weights()[0][14,:]

"""#### Create and apply an `Embedding` layer that uses `mask_zero=True`"""

# Create a layer that uses the mask_zero kwarg

masking_embedding_layer = tf.keras.layers.Embedding(input_dim=501, output_dim=16, mask_zero=True)

# Apply this layer to the sequence and see the _keras_mask property

masked_sequence_of_embeddings = masking_embedding_layer(sequence_of_indices)
masked_sequence_of_embeddings._keras_mask

"""---
<a id="coding_tutorial_4"></a>
## The Embedding Projector

#### Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### Load and preprocess the IMDb data"""

# A function to load and preprocess the IMDB dataset

def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):
    from tensorflow.keras.datasets import imdb

    # Load the reviews
    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',
                                                          num_words=num_words,
                                                          skip_top=0,
                                                          maxlen=maxlen,
                                                          start_char=1,
                                                          oov_char=2,
                                                          index_from=index_from)

    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,
                                                        maxlen=None,
                                                        padding='pre',
                                                        truncating='pre',
                                                        value=0)
    
    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,
                                                           maxlen=None,
                                                           padding='pre',
                                                           truncating='pre',
                                                           value=0)
    return (x_train, y_train), (x_test, y_test)

# Load the dataset

(x_train, y_train), (x_test, y_test) = get_and_pad_imdb_dataset()

# A function to get the dataset word index

def get_imdb_word_index(num_words=10000, index_from=2):
    imdb_word_index = tf.keras.datasets.imdb.get_word_index(
                                        path='imdb_word_index.json')
    imdb_word_index = {key: value + index_from for
                       key, value in imdb_word_index.items() if value <= num_words-index_from}
    return imdb_word_index

# Get the word index

imdb_word_index = get_imdb_word_index()

# imdb_word_index

# Swap the keys and values of the word index

inv_imdb_word_index = {value:key for key, value in imdb_word_index.items()}

# View the first dataset example sentence

[inv_imdb_word_index[index] for index in x_train[100] if index > 2]

"""#### Build an Embedding layer into a model"""

# Get the maximum token value

max_index_value = max(imdb_word_index.values())

# Specify an embedding dimension

embedding_dim = 16

# Build a model using Sequential:
#     1. Embedding layer
#     2. GlobalAveragePooling1D
#     3. Dense

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling1D, Dense, Embedding

model = Sequential([
                    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False),
                    GlobalAveragePooling1D(),
                    Dense(1, activation='sigmoid')
])

# Functional API refresher: use the Model to build the same model
review_sequence = tf.keras.layers.Input((None,))
embedding_sequence = Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False)(review_sequence)
average_embedding = GlobalAveragePooling1D()(embedding_sequence)
positive_prob = Dense(1, activation='sigmoid')(average_embedding)

model = tf.keras.Model(inputs=review_sequence, outputs=positive_prob)

model.summary()

"""#### Compile, train, and evaluate the model"""

# Compile the model with a binary cross-entropy loss

model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')

# Train the model using .fit(), savng its history

history = model.fit(x_train, y_train, epochs=5, batch_size=32, 
                    validation_data=(x_test,y_test), validation_steps=20)

# Commented out IPython magic to ensure Python compatibility.
# Plot the training and validation accuracy

import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')

history_dict = history.history

acc      = history_dict['accuracy']
val_acc  = history_dict['val_accuracy']
loss     = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(14,5))
plt.plot(epochs, acc, marker='.', label='Training acc')
plt.plot(epochs, val_acc, marker='.', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Classification accuracy')
plt.legend(loc='lower right')
plt.ylim(0, 1);

"""#### The TensorFlow embedding projector

The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/).
"""

imdb_word_index.items()

# Retrieve the embedding layer's weights from the trained model

weights = model.layers[1].get_weights()[0]
weights[1410]

# Save the word Embeddings to tsv files
# Two files: 
#     one contains the embedding labels (meta.tsv),
#     one contains the embeddings (vecs.tsv)

import io
import os
# from os import path
path = '/content/drive/My Drive/INSAID/TensorFlow/Customizing your models with Tensorflow 2/Week3/notebooks/data'
out_v = io.open(os.path.join(path, 'vecs.tsv'), 'w', encoding='utf-8')
out_m = io.open(os.path.join(path, 'meta.tsv'), 'w', encoding='utf-8')

k = 0

for word, token in imdb_word_index.items():
    if k != 0:
        out_m.write('\n')
        out_v.write('\n')
    
    out_v.write('\t'.join([str(x) for x in weights[token]]))
    out_m.write(word)
    k += 1
    
out_v.close()
out_m.close()
# beware large collections of embeddings!

"""---
<a id="coding_tutorial_5"></a>
## Recurrent neural network layers

#### Initialize and pass an input to a SimpleRNN layer
"""

# Create a SimpleRNN layer and test it

simplernn_layer = tf.keras.layers.SimpleRNN(units=16)

# Note that only the final cell output is returned

sequence = tf.constant([[[1.,1.],[2.,2.],[3.,3,]]])
layer_output = simplernn_layer(sequence)
layer_output

sequence

import numpy as np
inputs = np.random.random([32, 10, 8]).astype(np.float32)
simple_rnn = tf.keras.layers.SimpleRNN(4)

output = simple_rnn(inputs)  # The output has shape `[32, 4]`.
print(output.shape)

simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = simple_rnn(inputs)
print(whole_sequence_output.shape)
print(final_state.shape)
inputs.shape

"""#### Load and transform the IMDB review sentiment dataset"""

# A function to load and preprocess the IMDB dataset

def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):
    from tensorflow.keras.datasets import imdb

    # Load the reviews
    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',
                                                          num_words=num_words,
                                                          skip_top=0,
                                                          maxlen=maxlen,
                                                          start_char=1,
                                                          oov_char=2,
                                                          index_from=index_from)

    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,
                                                        maxlen=None,
                                                        padding='pre',
                                                        truncating='pre',
                                                        value=0)
    
    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,
                                                           maxlen=None,
                                                           padding='pre',
                                                           truncating='pre',
                                                           value=0)
    return (x_train, y_train), (x_test, y_test)

# Load the dataset

(x_train, y_train), (x_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)

# A function to get the dataset word index

def get_imdb_word_index(num_words=10000, index_from=2):
    imdb_word_index = tf.keras.datasets.imdb.get_word_index(
                                        path='imdb_word_index.json')
    imdb_word_index = {key: value + index_from for
                       key, value in imdb_word_index.items() if value <= num_words-index_from}
    return imdb_word_index

# Get the word index using get_imdb_word_index()

imdb_word_index = get_imdb_word_index()

"""#### Create a recurrent neural network model"""

# Get the maximum index value

max_index_value = max(imdb_word_index.values())
embedding_dim = 16

# Using Sequential, build the model:
# 1. Embedding.
# 2. LSTM.
# 3. Dense.

model = tf.keras.Sequential([
                             tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),
                             tf.keras.layers.LSTM(units=16),
                             tf.keras.layers.Dense(1, activation='sigmoid')
])

"""#### Compile and fit the model"""

# Compile the model with binary cross-entropy loss

model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')

# Fit the model and save its training history

history = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=3, batch_size=32)

"""#### Plot learning curves"""

# Commented out IPython magic to ensure Python compatibility.
# Plot the training and validation accuracy

import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')

history_dict = history.history

acc      = history_dict['accuracy']
val_acc  = history_dict['val_accuracy']
loss     = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(14,5))
plt.plot(epochs, acc, marker='.', label='Training acc')
plt.plot(epochs, val_acc, marker='.', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Classification accuracy')
plt.legend(loc='lower right')
plt.ylim(0, 1);

"""#### Make predictions with the model"""

# View the first test data example sentence
# (invert the word index)

inv_imdb_word_index = {value:key for key, value in imdb_word_index.items()}
print([inv_imdb_word_index[index] for index in x_test[0] if index>2])

# Get the model prediction using model.predict()
print('predict for : ', x_test[None, 0, :])
pred = model.predict(x_test[None, 0, :])
print(f'predicted {pred}')

# Get the corresponding label

print(f'actual label : {y_test[0]}')

"""---
<a id="coding_tutorial_6"></a>
## Stacked RNNs and the Bidirectional wrapper

#### Load and transform the IMDb review sentiment dataset
"""

# A function to load and preprocess the IMDB dataset

def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):
    from tensorflow.keras.datasets import imdb

    # Load the reviews
    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',
                                                          num_words=num_words,
                                                          skip_top=0,
                                                          maxlen=maxlen,
                                                          start_char=1,
                                                          oov_char=2,
                                                          index_from=index_from)

    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,
                                                        maxlen=None,
                                                        padding='pre',
                                                        truncating='pre',
                                                        value=0)
    
    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,
                                                           maxlen=None,
                                                           padding='pre',
                                                           truncating='pre',
                                                           value=0)
    return (x_train, y_train), (x_test, y_test)

# Load the dataset

(x_train, y_train), (x_test, y_test) = get_and_pad_imdb_dataset(num_words=5000, maxlen=250)

# A function to get the dataset word index

def get_imdb_word_index(num_words=10000, index_from=2):
    imdb_word_index = tf.keras.datasets.imdb.get_word_index(
                                        path='imdb_word_index.json')
    imdb_word_index = {key: value + index_from for
                       key, value in imdb_word_index.items() if value <= num_words-index_from}
    return imdb_word_index

# Get the word index using get_imdb_word_index()

imdb_word_index = get_imdb_word_index(5000)

"""#### Build stacked and bidirectional recurrent models"""

# Get the maximum index value and specify an embedding dimension

max_index_value = max(imdb_word_index.values())
embedding_dim = 16

# Using Sequential, build a stacked LSTM model via return_sequences=True

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),
                                    tf.keras.layers.LSTM(units=32, return_sequences=True),
                                    tf.keras.layers.LSTM(units=32, return_sequences=False),
                                    tf.keras.layers.Dense(1, activation='sigmoid')                             
])

model.summary()

# Using Sequential, build a bidirectional RNN with merge_mode='sum'

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),
                                    tf.keras.layers.Bidirectional(layer = tf.keras.layers.LSTM(units=32), merge_mode='sum',
                                                                  backward_layer=tf.keras.layers.GRU(units=32, go_backwards=True)),
                                    tf.keras.layers.Dense(1, activation='sigmoid')                             
])

for layer in model.layers:
    print(layer)
model.summary()

# Create a model featuring both stacked recurrent layers and a bidirectional layer

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),
                                    tf.keras.layers.Bidirectional(layer = tf.keras.layers.LSTM(units=32, return_sequences=True), 
                                                                  merge_mode='concat'),
                                    tf.keras.layers.GRU(units=8, return_sequences=False),
                                    tf.keras.layers.Dense(1, activation='sigmoid')                             
])
for layer in model.layers:
    print(layer)
print("\n")    
model.summary()

"""#### Compile and fit the model"""

# Compile the model

model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')

# Train the model, saving its history
history={}
with tf.device('GPU:0'):
    history = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test,y_test))

# Commented out IPython magic to ensure Python compatibility.
# Plot the training and validation accuracy

import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')

history_dict = history.history

acc      = history_dict['accuracy']
val_acc  = history_dict['val_accuracy']
loss     = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(14,5))
plt.plot(epochs, acc, marker='.', label='Training acc')
plt.plot(epochs, val_acc, marker='.', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Classification accuracy')
plt.legend(loc='lower right')
plt.ylim(0, 1);