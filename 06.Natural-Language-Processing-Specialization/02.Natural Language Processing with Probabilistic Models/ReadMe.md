## Natural Language Processing with Probabilistic Models
- Create a simple auto-correct algorithm using minimum edit distance and dynamic programming,
- Apply the Viterbi Algorithm for part-of-speech (POS) tagging, which is important for computational linguistics.
- Write a better auto-complete algorithm using an N-gram language model.
- Write your own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.

### Week 1
Autocorrect, minimum edit distance, and dynamic programming<br>
Build your own spellchecker to correct misspelled words!
- Word probabilities
- Dynamic programming
- Minimum edit distance
- Autocorrect

### Week 2
Markov chains and Hidden Markov models, to create part-of-speech tags for a Wall Street Journal text corpus!
- Markov chains
- Hidden Markov models
- Part-of-speech tagging
- Viterbi algorithm
- Transition probabilities
- Emission probabilities

### Week 3
How N-gram language models work by calculating sequence probabilities.
Build autocomplete language model using a text corpus from Twitter!
- Conditional probabilities
- Text pre-processing
- Language modeling
- Perplexity
- K-smoothing
- N-grams
- Backoff
- Tokenization