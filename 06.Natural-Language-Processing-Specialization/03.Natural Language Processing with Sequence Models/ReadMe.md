## Natural Language Processing with Sequence Models
- Train a neural network with GLoVe word embeddings to perform sentiment analysis of tweets.
- Generate synthetic Shakespeare text using a Gated Recurrent Unit (GRU) language model.
- Train a recurrent neural network to perform named entity recognition (NER) using LSTMs with linear layers.
- Use so-called ‘Siamese’ LSTM models to compare questions in a corpus and identify those that are worded differently but have the same meaning.

### Week 1
Build a sophisticated tweet classifier that places tweets into positive or negative sentiment categories, using a deep neural network.
- Feature extraction
- Supervised machine learning
- Binary classification
- Text preprocessing
- ReLU
- Python classes
- Trax
- Neural networks

### Week 2
Limitations of traditional language models and how RNNs and GRUs use sequential data for text prediction. 
Build next-word generator using a simple RNN on Shakespeare text data!
- N-grams
- Gated recurrent units
- Recurrent neural networks

### Week 3
How long short-term memory units (LSTMs) solve the vanishing gradient problem,
How Named Entity Recognition systems quickly extract important information from text. 
Build Named Entity Recognition system using an LSTM and data from Kaggle!
- Vanishing gradients
- Named entity recognition
- LSTMs
- Feature extraction
- Part-of-speech tagging
- Data generators

### Week 4
Siamese networks, a special type of neural network made of two identical networks that are eventually merged together.
Build Siamese network that identifies question duplicates in a dataset from Quora.
- One shot learning
- Triplet loss
- Cosine similarity
- Siamese networks
- Data generators